# Print specific columns
awk '{print $1}' file.txt              # First column
awk '{print $1, $3}' file.txt          # First and third columns
awk '{print $NF}' file.txt             # Last column
awk '{print $(NF-1)}' file.txt         # Second to last column
-----
# Custom field separator
awk -F',' '{print $1}' file.csv        # CSV
awk -F':' '{print $1, $3}' /etc/passwd # Colon separated
awk -F'\t' '{print $2}' file.tsv       # Tab separated
-----
# Pattern matching
awk '/pattern/ {print $0}' file.txt    # Lines matching pattern
awk '!/pattern/ {print $0}' file.txt   # Lines NOT matching pattern
awk '/start/,/end/ {print}' file.txt   # Between patterns
-----
# Conditional operations
awk '$3 > 100 {print $1}' file.txt     # If column 3 > 100
awk '$1 == "foo" {print $2}' file.txt  # If column 1 equals foo
awk 'NR > 1 {print}' file.txt          # Skip first line (header)
awk 'NF > 0 {print}' file.txt          # Skip empty lines
-----
# Calculations
awk '{sum += $1} END {print sum}' file.txt        # Sum column 1
awk '{sum += $1; count++} END {print sum/count}' file.txt  # Average
awk '{if ($1 > max) max = $1} END {print max}' file.txt    # Max value
awk '{total += $2; print $1, total}' file.txt     # Running total
-----
# String operations
awk '{print length($0)}' file.txt      # Line length
awk 'length($0) > 80 {print}' file.txt # Lines longer than 80 chars
awk '{print toupper($0)}' file.txt     # Convert to uppercase
awk '{print tolower($0)}' file.txt     # Convert to lowercase
-----
# Multiple conditions
awk '$3 > 100 && $4 < 200 {print}' file.txt
awk '$1 == "foo" || $1 == "bar" {print}' file.txt
-----
# BEGIN and END blocks
awk 'BEGIN {print "Report"} {print $1} END {print "Total:", NR}' file.txt
awk 'BEGIN {FS=","; OFS="\t"} {print $1, $2}' file.csv
-----
# Add line numbers
awk '{print NR, $0}' file.txt
-----
# Remove duplicates
awk '!seen[$0]++' file.txt
awk '!seen[$1]++' file.txt             # Based on first column
-----
# Format output
awk '{printf "%-20s %10s\n", $1, $2}' file.txt
awk '{printf "%s,%s,%s\n", $1, $2, $3}' file.txt
-----
# Process CSV with quoted fields
awk -F'","' '{gsub(/^"|"$/, ""); print $2}' file.csv
-----
# Count occurrences
awk '{count[$1]++} END {for (word in count) print word, count[word]}' file.txt
-----
# Join lines
awk '{printf "%s ", $0} END {print ""}' file.txt
awk 'ORS=" " {print}' file.txt
-----
# Split and process
awk -F'@' '{print "User:", $1, "Domain:", $2}' emails.txt
-----
# Filter by column value
awk -F',' '$3 == "active" {print $1, $2}' users.csv
-----
# Replace field value
awk '{$2 = "newvalue"; print}' file.txt
awk -F',' -v OFS=',' '{$3 = "updated"; print}' file.csv
-----
# Extract fields by pattern
awk -F'=' '/config/ {print $2}' config.txt
-----
# Process log files
awk '$9 == 404 {print $1, $7}' access.log    # 404 errors with IP and path
awk '{ip[$1]++} END {for (i in ip) print i, ip[i]}' access.log  # Count IPs
-----
# Date filtering (assuming date in column 4)
awk '$4 >= "2024-01-01" && $4 <= "2024-12-31" {print}' log.txt
-----
# Multi-line records
awk 'BEGIN {RS=""; FS="\n"} {print $1}' file.txt
-----
# Transpose data
awk '{for(i=1;i<=NF;i++) a[i,NR]=$i; max=(NF>max?NF:max)} END {for(i=1;i<=max;i++) {for(j=1;j<=NR;j++) printf "%s%s", a[i,j], (j<NR?OFS:ORS)}}' file.txt
-----
# Merge lines based on condition
awk '/pattern/ {printf "%s ", $0; next} {print}' file.txt
-----
# Print every nth line
awk 'NR % 5 == 0' file.txt            # Every 5th line
-----
# Compare columns
awk '$1 != $2 {print}' file.txt       # Where column 1 != column 2
-----
# Custom functions
awk 'function abs(x){return x<0 ? -x : x} {print abs($1)}' file.txt
